{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training a NN on MNIST dataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYL9qNLM3FqEnkloE1IP7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yootazi/Training-_a_NN_on_MNIST_dataset/blob/main/Training_a_NN_on_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QL66_PEsnUp"
      },
      "source": [
        "**I- Downloading the MNIST Dataset**\n",
        "\n",
        "**II- Creating a Data Loader**\n",
        "\n",
        "**III- Building a Model**\n",
        "\n",
        "**IV- Training the Model**\n",
        "\n",
        "**V- Saving the Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "SYStXeWZbbM8"
      },
      "source": [
        "#@title **Connecting to Google Drive** { form-width: \"50%\" }\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ngno4SGXaRr-"
      },
      "source": [
        "#@title **Importing Libraries** { form-width: \"50%\" }\n",
        "!pip install torchaudio librosa boto3\n",
        "\n",
        "import torchvision   # torchvision is a package in the PyTorch library containing computer-vision models, datasets, and image transformations\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets    \n",
        "from torchvision.transforms import ToTensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "DwgnLCanaMXu"
      },
      "source": [
        "#@title **Creating and Training the Model with the following parameters:** { form-width: \"50%\" }\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.001 #@param {type:\"raw\"}\n",
        "EPOCHS =  10 #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "class FeedForwardNet(nn.Module):                                         # Building the model - inherits from imported nn.Module\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()           # storing input layer of the model in this class\n",
        "        self.dense_layers = nn.Sequential(    # storing hidden and output layers packed together (nn.Sequential)\n",
        "            nn.Linear(28 * 28, 256),          # dense layer (input features(MNIST measures), output features)\n",
        "            nn.ReLU(),                        # applying activation function\n",
        "            nn.Linear(256, 10)                # output layer(number of input, number of output)\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)      # nn.Softmax transforms values in a way that the sum == 1 (normalisation)\n",
        "\n",
        "    def forward(self, input_data):            # defining the forward method to tell pytorch in what sequence it should manipilate the data\n",
        "        x = self.flatten(input_data)\n",
        "        logits = self.dense_layers(x)         # passing input data to dense layer\n",
        "        predictions = self.softmax(logits)    # normalising the output data\n",
        "        return predictions\n",
        "\n",
        "\n",
        "def download_mnist_datasets():                                           # Load MNIST Dataset (70,000 handwritten numeric digit images and their respective labels- 60000 train_set, 10000 test_set) from PyTorch Torchvision\n",
        "   \n",
        "    train_data = datasets.MNIST(                        # train set\n",
        "        root=\"/content/gdrive/MyDrive/musicdata/\",      # where to store the dataset\n",
        "        train=True,                                     # we want the train set of the dataset\n",
        "        download=True,\n",
        "        transform=ToTensor(),                           # ToTensor reshapes a new tensor where each value is normalised between 0 and 1\n",
        "    )\n",
        "    validation_data = datasets.MNIST(                   # test set\n",
        "        root=\"/content/gdrive/MyDrive/musicdata/\",\n",
        "        train=False,                                    # we want the non-trained part of the dataset (won't be used in training our model)\n",
        "        download=True,\n",
        "        transform=ToTensor(),\n",
        "    )\n",
        "    return train_data, validation_data\n",
        "  \n",
        "\n",
        "def create_data_loader(train_data, batch_size):                          # DataLoader fetches data in batches - combines the dataset and a sampler, returning an iterable over the dataset\n",
        "    \n",
        "    train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)    # BATCH_SIZE is defined in the previous cell\n",
        "    return train_dataloader\n",
        "\n",
        "\n",
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):    \n",
        "    for input, target in data_loader:                                    # looping through all the samples in the dataset getting a new batch of samples in each iteration returning x == input and y == target for one batch\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        prediction = model(input)                                        # calculating loss\n",
        "        loss = loss_fn(prediction, target)                               # loos_fn will be defined later\n",
        "\n",
        "        optimiser.zero_grad()                                            # backpropagate error and update weights (reseting grediant to zero for each training iteration == zero_grad)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print(f\"loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):    # will go through all epochs\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # download data and create data loader\n",
        "    train_data, _ = download_mnist_datasets()     # not using validation_data (_)\n",
        "    print(\"MNIST dataset downloaded\")\n",
        "\n",
        "    train_dataloader = create_data_loader(train_data, BATCH_SIZE)\n",
        "    \n",
        "\n",
        "    # construct model and assign it to device\n",
        "    if torch.cuda.is_available():    # checking if gpu acceleration is available\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    print(f\"Using {device}\")\n",
        "    feed_forward_net = FeedForwardNet().to(device)\n",
        "    print(feed_forward_net)\n",
        "\n",
        "    # initialise loss funtion + optimiser\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.Adam(feed_forward_net.parameters(),\n",
        "                                 lr=LEARNING_RATE)\n",
        "\n",
        "    # train model\n",
        "    train(feed_forward_net, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "    # save model\n",
        "    path = '/content/gdrive/MyDrive/ai_music_projects/training_NN_on_MNIST_dataset'\n",
        "    torch.save(feed_forward_net.state_dict(), \"/content/gdrive/MyDrive/ai_music_projects/training_NN_on_MNIST_dataset/feedforward.pth\")\n",
        "    print(\"Trained feed forward net saved in {}\".format(path))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}